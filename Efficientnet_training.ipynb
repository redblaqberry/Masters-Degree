{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f'Using {device} for inference')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-10T13:06:55.795650Z","iopub.execute_input":"2024-02-10T13:06:55.796041Z","iopub.status.idle":"2024-02-10T13:07:02.992437Z","shell.execute_reply.started":"2024-02-10T13:06:55.796010Z","shell.execute_reply":"2024-02-10T13:07:02.991383Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using cuda for inference\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## MODEL TRAINING","metadata":{}},{"cell_type":"code","source":"!pip install torchviz\n!pip install torchsummary\n!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-02-10T13:22:54.005363Z","iopub.execute_input":"2024-02-10T13:22:54.005824Z","iopub.status.idle":"2024-02-10T13:23:23.083672Z","shell.execute_reply.started":"2024-02-10T13:22:54.005780Z","shell.execute_reply":"2024-02-10T13:23:23.082425Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting torchviz\n  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.1.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\nBuilding wheels for collected packages: torchviz\n  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=c1318eadb0c6b2101805b72e7fb4aa09e3bbdf13fe0382d693729b394c1385a5\n  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\nSuccessfully built torchviz\nInstalling collected packages: torchviz\nSuccessfully installed torchviz-0.0.2\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision.datasets as datasets\nnormalize= transforms.Normalize(mean=[0.49186882, 0.48265398, 0.44717732], std=[0.24697122, 0.24338895, 0.2615926 ])\ntransform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n        # transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.49186882, 0.48265398, 0.44717732],std=[0.24697122, 0.24338895, 0.2615926 ])\n    ])\n\nbatch_size = 64\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomCrop(32, 4),\n            transforms.ToTensor(),\n            normalize,\n        ]), download=True),\n        batch_size=batch_size, shuffle=True, pin_memory=True)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n        ]), download=True),\n        batch_size=batch_size, shuffle=True, pin_memory=True)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","metadata":{"execution":{"iopub.status.busy":"2024-02-10T21:00:47.617137Z","iopub.execute_input":"2024-02-10T21:00:47.617546Z","iopub.status.idle":"2024-02-10T21:00:50.510512Z","shell.execute_reply.started":"2024-02-10T21:00:47.617507Z","shell.execute_reply":"2024-02-10T21:00:50.509519Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_widese_b4', pretrained=False)\nmodel.eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:58:28.553888Z","iopub.execute_input":"2024-02-10T19:58:28.554280Z","iopub.status.idle":"2024-02-10T19:58:29.147938Z","shell.execute_reply.started":"2024-02-10T19:58:28.554250Z","shell.execute_reply":"2024-02-10T19:58:29.147034Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n","output_type":"stream"},{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"EfficientNet(\n  (stem): Sequential(\n    (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (activation): SiLU(inplace=True)\n  )\n  (layers): Sequential(\n    (0): Sequential(\n      (block0): MBConvBlock(\n        (depsep): Sequential(\n          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n          (bn): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=48, out_features=12, bias=True)\n          (expand): Linear(in_features=12, out_features=48, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (depsep): Sequential(\n          (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=24, out_features=6, bias=True)\n          (expand): Linear(in_features=6, out_features=24, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (1): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=144, out_features=36, bias=True)\n          (expand): Linear(in_features=36, out_features=144, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=192, out_features=48, bias=True)\n          (expand): Linear(in_features=48, out_features=192, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block2): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=192, out_features=48, bias=True)\n          (expand): Linear(in_features=48, out_features=192, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block3): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=192, out_features=48, bias=True)\n          (expand): Linear(in_features=48, out_features=192, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (2): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=192, out_features=48, bias=True)\n          (expand): Linear(in_features=48, out_features=192, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=336, out_features=84, bias=True)\n          (expand): Linear(in_features=84, out_features=336, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block2): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=336, out_features=84, bias=True)\n          (expand): Linear(in_features=84, out_features=336, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block3): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=336, out_features=84, bias=True)\n          (expand): Linear(in_features=84, out_features=336, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (3): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(336, 336, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=336, bias=False)\n          (bn): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=336, out_features=84, bias=True)\n          (expand): Linear(in_features=84, out_features=336, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block2): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block3): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block4): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block5): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (4): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=672, out_features=168, bias=True)\n          (expand): Linear(in_features=168, out_features=672, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block2): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block3): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block4): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block5): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (5): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(960, 960, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=960, bias=False)\n          (bn): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=960, out_features=240, bias=True)\n          (expand): Linear(in_features=240, out_features=960, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block2): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block3): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block4): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block5): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block6): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n      (block7): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n    (6): Sequential(\n      (block0): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n          (bn): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=1632, out_features=408, bias=True)\n          (expand): Linear(in_features=408, out_features=1632, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_quantizer): Identity()\n      )\n      (block1): MBConvBlock(\n        (expand): Sequential(\n          (conv): Conv2d(448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (depsep): Sequential(\n          (conv): Conv2d(2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2688, bias=False)\n          (bn): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (se): SequentialSqueezeAndExcitation(\n          (squeeze): Linear(in_features=2688, out_features=672, bias=True)\n          (expand): Linear(in_features=672, out_features=2688, bias=True)\n          (activation): SiLU(inplace=True)\n          (sigmoid): Sigmoid()\n          (mul_a_quantizer): Identity()\n          (mul_b_quantizer): Identity()\n        )\n        (proj): Sequential(\n          (conv): Conv2d(2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        )\n        (residual_add): StochasticDepthResidual()\n        (residual_quantizer): Identity()\n      )\n    )\n  )\n  (features): Sequential(\n    (conv): Conv2d(448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (activation): SiLU(inplace=True)\n  )\n  (classifier): Sequential(\n    (pooling): AdaptiveAvgPool2d(output_size=1)\n    (squeeze): Flatten()\n    (dropout): Dropout(p=0.4, inplace=False)\n    (fc): Linear(in_features=1792, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"num_epochs = 100\nlearning_rate = 0.0005\nweight_decay = 0.0001\nmomentum = 0.01","metadata":{"execution":{"iopub.status.busy":"2024-02-10T23:19:26.656151Z","iopub.execute_input":"2024-02-10T23:19:26.656515Z","iopub.status.idle":"2024-02-10T23:19:26.661377Z","shell.execute_reply.started":"2024-02-10T23:19:26.656485Z","shell.execute_reply":"2024-02-10T23:19:26.660269Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40])\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-02-10T23:19:27.776192Z","iopub.execute_input":"2024-02-10T23:19:27.776669Z","iopub.status.idle":"2024-02-10T23:19:27.788895Z","shell.execute_reply.started":"2024-02-10T23:19:27.776623Z","shell.execute_reply":"2024-02-10T23:19:27.787815Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/working/ef_net_78.pth\")\nmodel.load_state_dict(training_stats['model_state_dict'])\noptimizer.load_state_dict(training_stats['optimizer_state_dict'])\n\nepoch = training_stats['epoch']\nloss = training_stats['loss']\naccuracy = training_stats['accuracy']\ntest_accuracy = training_stats['test_accuracy']\ntest_loss = training_stats['test_loss']\nlearning_rate = training_stats['learning_rate']","metadata":{"execution":{"iopub.status.busy":"2024-02-11T00:24:33.483215Z","iopub.execute_input":"2024-02-11T00:24:33.484110Z","iopub.status.idle":"2024-02-11T00:24:33.819020Z","shell.execute_reply.started":"2024-02-11T00:24:33.484078Z","shell.execute_reply":"2024-02-11T00:24:33.818073Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:28:12.871910Z","iopub.execute_input":"2024-02-10T19:28:12.872560Z","iopub.status.idle":"2024-02-10T19:28:12.876819Z","shell.execute_reply.started":"2024-02-10T19:28:12.872529Z","shell.execute_reply":"2024-02-10T19:28:12.875938Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"0.17334\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nfor epoch in range(50):\n    model.train()\n    total_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    data_loader = tqdm(trainloader, total=len(trainloader), desc=f'Epoch [{epoch + 1}/{num_epochs}]')\n\n    for batch_idx, (inputs, targets) in enumerate(data_loader):\n        optimizer.zero_grad()\n        inputs, targets = inputs.to(device), targets.to(device)\n\n\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        _, predicted = outputs.max(1)\n        correct_predictions += predicted.eq(targets).sum().item()\n        total_samples += targets.size(0)\n\n        # Progress bar description\n        data_loader.set_postfix(loss=total_loss / (batch_idx + 1), accuracy=correct_predictions / total_samples)\n\n    average_loss = total_loss / len(trainloader)\n    accuracy = correct_predictions / total_samples\n\n    # test the model\n    correct_test, total_test, total_loss_test = 0, 0, 0\n    with torch.no_grad():\n      for (inputs, targets) in testloader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        total_loss_test += loss.item()\n\n        __, predicted = outputs.max(1)\n        correct_test += predicted.eq(targets).sum().item()\n        total_test+= targets.size(0)\n\n      accuracy_test = correct_test / total_test\n      average_loss_test = total_loss_test / len(testloader)\n\n    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%, Loss on test data: {average_loss_test:.4f} Accuracy on test data: {100 * accuracy_test:.2f} %')\n    model_save_path = 'New_efficientnet_cifar10_' + str(epoch+1) + '.pth'\n    training_stats = {\n      'epoch': epoch+1,\n      'model_state_dict': model.state_dict(),\n      'optimizer_state_dict': optimizer.state_dict(),\n      'scheduler_state_dict': scheduler.state_dict(),\n      'loss': average_loss,\n      'accuracy': accuracy,\n      'test_accuracy': accuracy_test,\n      'test_loss': average_loss_test,\n      'learning_rate': optimizer.param_groups[0][\"lr\"]\n    }\n    scheduler.step()\n    if(epoch%10==0):\n        torch.save(training_stats, model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T23:19:29.838309Z","iopub.execute_input":"2024-02-10T23:19:29.838678Z","iopub.status.idle":"2024-02-11T00:23:21.430136Z","shell.execute_reply.started":"2024-02-10T23:19:29.838648Z","shell.execute_reply":"2024-02-11T00:23:21.428374Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stderr","text":"Epoch [1/100]: 100%|| 782/782 [01:37<00:00,  8.06it/s, accuracy=0.821, loss=0.512]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/100] - Loss: 0.5116, Accuracy: 82.07%, Loss on test data: 0.6649 Accuracy on test data: 77.87 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/100]: 100%|| 782/782 [01:36<00:00,  8.07it/s, accuracy=0.821, loss=0.506]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/100] - Loss: 0.5065, Accuracy: 82.07%, Loss on test data: 0.6663 Accuracy on test data: 77.33 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/100]: 100%|| 782/782 [01:38<00:00,  7.97it/s, accuracy=0.821, loss=0.505]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/100] - Loss: 0.5052, Accuracy: 82.13%, Loss on test data: 0.6708 Accuracy on test data: 77.70 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/100]: 100%|| 782/782 [01:38<00:00,  7.91it/s, accuracy=0.823, loss=0.504]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/100] - Loss: 0.5038, Accuracy: 82.29%, Loss on test data: 0.6773 Accuracy on test data: 77.24 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/100]: 100%|| 782/782 [01:38<00:00,  7.92it/s, accuracy=0.826, loss=0.502]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/100] - Loss: 0.5018, Accuracy: 82.57%, Loss on test data: 0.6689 Accuracy on test data: 77.78 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/100]: 100%|| 782/782 [01:38<00:00,  7.92it/s, accuracy=0.824, loss=0.501]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/100] - Loss: 0.5006, Accuracy: 82.35%, Loss on test data: 0.6618 Accuracy on test data: 78.36 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/100]: 100%|| 782/782 [01:38<00:00,  7.93it/s, accuracy=0.825, loss=0.5]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/100] - Loss: 0.5002, Accuracy: 82.50%, Loss on test data: 0.6629 Accuracy on test data: 77.98 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/100]: 100%|| 782/782 [01:38<00:00,  7.91it/s, accuracy=0.825, loss=0.496]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/100] - Loss: 0.4958, Accuracy: 82.52%, Loss on test data: 0.6837 Accuracy on test data: 77.53 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/100]: 100%|| 782/782 [01:38<00:00,  7.94it/s, accuracy=0.827, loss=0.494]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/100] - Loss: 0.4938, Accuracy: 82.67%, Loss on test data: 0.6695 Accuracy on test data: 78.30 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/100]: 100%|| 782/782 [01:37<00:00,  8.03it/s, accuracy=0.827, loss=0.493]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/100] - Loss: 0.4927, Accuracy: 82.67%, Loss on test data: 0.6615 Accuracy on test data: 78.29 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/100]: 100%|| 782/782 [01:35<00:00,  8.22it/s, accuracy=0.828, loss=0.489]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/100] - Loss: 0.4891, Accuracy: 82.80%, Loss on test data: 0.6652 Accuracy on test data: 77.99 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/100]: 100%|| 782/782 [01:36<00:00,  8.09it/s, accuracy=0.829, loss=0.49] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/100] - Loss: 0.4902, Accuracy: 82.90%, Loss on test data: 0.6724 Accuracy on test data: 77.40 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/100]: 100%|| 782/782 [01:34<00:00,  8.24it/s, accuracy=0.827, loss=0.491]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/100] - Loss: 0.4909, Accuracy: 82.70%, Loss on test data: 0.6621 Accuracy on test data: 77.79 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/100]: 100%|| 782/782 [01:34<00:00,  8.27it/s, accuracy=0.826, loss=0.492]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/100] - Loss: 0.4922, Accuracy: 82.64%, Loss on test data: 0.6779 Accuracy on test data: 77.97 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/100]: 100%|| 782/782 [01:34<00:00,  8.24it/s, accuracy=0.829, loss=0.482]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/100] - Loss: 0.4825, Accuracy: 82.88%, Loss on test data: 0.6625 Accuracy on test data: 77.99 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/100]: 100%|| 782/782 [01:35<00:00,  8.23it/s, accuracy=0.828, loss=0.488]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/100] - Loss: 0.4879, Accuracy: 82.82%, Loss on test data: 0.6571 Accuracy on test data: 78.47 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/100]: 100%|| 782/782 [01:34<00:00,  8.25it/s, accuracy=0.829, loss=0.484]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/100] - Loss: 0.4844, Accuracy: 82.89%, Loss on test data: 0.6797 Accuracy on test data: 77.52 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/100]: 100%|| 782/782 [01:34<00:00,  8.24it/s, accuracy=0.832, loss=0.476]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/100] - Loss: 0.4755, Accuracy: 83.23%, Loss on test data: 0.6581 Accuracy on test data: 78.12 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.832, loss=0.476]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/100] - Loss: 0.4763, Accuracy: 83.20%, Loss on test data: 0.6712 Accuracy on test data: 77.86 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.832, loss=0.478]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/100] - Loss: 0.4785, Accuracy: 83.25%, Loss on test data: 0.6766 Accuracy on test data: 77.94 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/100]: 100%|| 782/782 [01:35<00:00,  8.21it/s, accuracy=0.836, loss=0.466]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/100] - Loss: 0.4656, Accuracy: 83.58%, Loss on test data: 0.6642 Accuracy on test data: 78.42 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/100]: 100%|| 782/782 [01:34<00:00,  8.28it/s, accuracy=0.834, loss=0.47] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/100] - Loss: 0.4701, Accuracy: 83.41%, Loss on test data: 0.6621 Accuracy on test data: 78.01 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.836, loss=0.467]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/100] - Loss: 0.4665, Accuracy: 83.57%, Loss on test data: 0.6548 Accuracy on test data: 78.34 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/100]: 100%|| 782/782 [01:34<00:00,  8.26it/s, accuracy=0.835, loss=0.472]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/100] - Loss: 0.4717, Accuracy: 83.49%, Loss on test data: 0.6586 Accuracy on test data: 78.21 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.835, loss=0.466]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/100] - Loss: 0.4659, Accuracy: 83.50%, Loss on test data: 0.6594 Accuracy on test data: 78.47 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [26/100]: 100%|| 782/782 [01:35<00:00,  8.21it/s, accuracy=0.834, loss=0.468]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [26/100] - Loss: 0.4684, Accuracy: 83.41%, Loss on test data: 0.6395 Accuracy on test data: 78.64 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [27/100]: 100%|| 782/782 [01:34<00:00,  8.29it/s, accuracy=0.837, loss=0.464]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [27/100] - Loss: 0.4638, Accuracy: 83.68%, Loss on test data: 0.6539 Accuracy on test data: 78.45 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [28/100]: 100%|| 782/782 [01:34<00:00,  8.29it/s, accuracy=0.838, loss=0.46] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [28/100] - Loss: 0.4604, Accuracy: 83.78%, Loss on test data: 0.6596 Accuracy on test data: 78.65 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [29/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.836, loss=0.469]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [29/100] - Loss: 0.4692, Accuracy: 83.60%, Loss on test data: 0.6589 Accuracy on test data: 78.55 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [30/100]: 100%|| 782/782 [01:34<00:00,  8.27it/s, accuracy=0.835, loss=0.467]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [30/100] - Loss: 0.4667, Accuracy: 83.50%, Loss on test data: 0.6673 Accuracy on test data: 78.21 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [31/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.835, loss=0.47] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [31/100] - Loss: 0.4702, Accuracy: 83.48%, Loss on test data: 0.6546 Accuracy on test data: 78.43 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [32/100]: 100%|| 782/782 [01:34<00:00,  8.29it/s, accuracy=0.833, loss=0.471]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [32/100] - Loss: 0.4709, Accuracy: 83.32%, Loss on test data: 0.6610 Accuracy on test data: 78.41 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [33/100]: 100%|| 782/782 [01:34<00:00,  8.27it/s, accuracy=0.835, loss=0.47] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [33/100] - Loss: 0.4699, Accuracy: 83.53%, Loss on test data: 0.6549 Accuracy on test data: 78.44 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [34/100]: 100%|| 782/782 [01:34<00:00,  8.28it/s, accuracy=0.834, loss=0.466]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [34/100] - Loss: 0.4662, Accuracy: 83.44%, Loss on test data: 0.6568 Accuracy on test data: 78.51 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [35/100]: 100%|| 782/782 [01:34<00:00,  8.27it/s, accuracy=0.836, loss=0.467]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [35/100] - Loss: 0.4670, Accuracy: 83.60%, Loss on test data: 0.6548 Accuracy on test data: 78.63 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [36/100]: 100%|| 782/782 [01:34<00:00,  8.30it/s, accuracy=0.838, loss=0.46] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [36/100] - Loss: 0.4603, Accuracy: 83.81%, Loss on test data: 0.6493 Accuracy on test data: 78.23 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [37/100]: 100%|| 782/782 [01:35<00:00,  8.15it/s, accuracy=0.837, loss=0.463]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [37/100] - Loss: 0.4627, Accuracy: 83.69%, Loss on test data: 0.6645 Accuracy on test data: 78.01 %\n","output_type":"stream"},{"name":"stderr","text":"Epoch [38/100]:  53%|    | 414/782 [00:51<00:45,  8.10it/s, accuracy=0.839, loss=0.467]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[148], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:233\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    232\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 233\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m    235\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:447\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepsep(x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand(x)))\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    446\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepsep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized:\n\u001b[1;32m    450\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_quantizer(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:248\u001b[0m, in \u001b[0;36mSequentialSqueezeAndExcitation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 248\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out \u001b[38;5;241m*\u001b[39m x\n","File \u001b[0;32m~/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:163\u001b[0m, in \u001b[0;36mSqueezeAndExcitation._attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand(out)\n\u001b[1;32m    162\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(out)\n\u001b[0;32m--> 163\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_stats = {\n    'epoch': epoch+1,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'loss': average_loss,\n    'accuracy': accuracy,\n    'test_accuracy': accuracy_test,\n    'test_loss': average_loss_test,\n    'learning_rate': optimizer.param_groups[0][\"lr\"]\n}\ntorch.save(training_stats, \"ef_net_78.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T00:23:30.059632Z","iopub.execute_input":"2024-02-11T00:23:30.060008Z","iopub.status.idle":"2024-02-11T00:23:30.543008Z","shell.execute_reply.started":"2024-02-11T00:23:30.059978Z","shell.execute_reply":"2024-02-11T00:23:30.541947Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy Train: \",accuracy)\nprint(\"Accuracy Test: \",accuracy_test)\nprint(\"Loss Train: \",average_loss)\nprint(\"Loss Test: \",average_loss_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T00:23:54.321895Z","iopub.execute_input":"2024-02-11T00:23:54.322288Z","iopub.status.idle":"2024-02-11T00:23:54.327734Z","shell.execute_reply.started":"2024-02-11T00:23:54.322257Z","shell.execute_reply":"2024-02-11T00:23:54.326833Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"Accuracy Train:  0.8369\nAccuracy Test:  0.7801\nLoss Train:  0.4627285508624733\nLoss Test:  0.6645258720133714\n","output_type":"stream"}]},{"cell_type":"code","source":"correct_predictions, total_samples = 0, 0\nwith torch.no_grad():\n  for (inputs, targets) in testloader:\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    __, predicted = outputs.max(1)\n    correct_predictions += predicted.eq(targets).sum().item()\n    total_samples += targets.size(0)\n\naccuracy = correct_predictions / total_samples\nprint(f'Accuracy on test data: {100 * correct_predictions / total_samples:.2f} %')","metadata":{"execution":{"iopub.status.busy":"2024-02-11T00:23:56.596378Z","iopub.execute_input":"2024-02-11T00:23:56.596748Z","iopub.status.idle":"2024-02-11T00:24:03.074772Z","shell.execute_reply.started":"2024-02-11T00:23:56.596717Z","shell.execute_reply":"2024-02-11T00:24:03.073803Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"Accuracy on test data: 78.15 %\n","output_type":"stream"}]}]}